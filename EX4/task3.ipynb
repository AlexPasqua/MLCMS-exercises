{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data - 60000 train, 10000 test\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_size = train_images.shape[0]\n",
    "test_size = test_images.shape[0]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalize images between 0 and 1\n",
    "f = lambda x: x / 255\n",
    "train_images = f(train_images)\n",
    "plt.imshow(train_images[0])\n",
    "train_images[0].shape\n",
    "\n",
    "train_images = tf.cast(train_images, tf.float32)\n",
    "\n",
    "# create dataset, shuffle and batch them\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                .shuffle(test_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"Variational autoencoder.\"\"\"\n",
    "    def __init__(self, latent_dim, batch_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        self.encoder.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1), batch_size=batch_size))\n",
    "        self.encoder.add(tf.keras.layers.Flatten())\n",
    "        self.encoder.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.encoder.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.encoder.add(tf.keras.layers.Dense(latent_dim + latent_dim, activation='sigmoid'))\n",
    "\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(tf.keras.layers.InputLayer(input_shape=(latent_dim,)))\n",
    "        self.decoder.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Dense(28*28, activation='sigmoid'))\n",
    "        self.decoder.add(tf.keras.layers.Reshape((28, 28)))\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    print(x_logit.shape, x.shape)\n",
    "    print(x_logit[0][0][0], x[0][0][0])\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# set the dimensionality of the latent space to a plane for visualization later\n",
    "latent_dim = 2\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = VAE(latent_dim, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.98811401e-11 8.81094919e-11\n",
      "   5.19382280e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.85493669e-11 3.43163276e-11\n",
      "   1.68799238e-10 2.27229749e-10 2.30939615e-10 2.35576947e-10\n",
      "   1.79001397e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   7.69798739e-11 1.50249868e-10 2.06825446e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.18882537e-10\n",
      "   9.46017709e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   1.66944306e-11 1.05731389e-10 1.40975190e-10 2.17955057e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.14245191e-10 1.46540002e-10 1.15933541e-10 5.00832917e-11\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.17788473e-10\n",
      "   2.11462778e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.12390258e-10 1.17788473e-10 7.23425278e-11\n",
      "   7.41974711e-12 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.39120252e-11 1.10368735e-10 2.20737470e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.26302282e-10 9.73841771e-11 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   1.94768351e-11 2.22592403e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 1.92913421e-10 6.77051887e-11\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   8.81094919e-11 2.28157215e-10 2.36504427e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.33722014e-10\n",
      "   1.75291517e-10 2.07752912e-10 2.35576947e-10 1.94768354e-10\n",
      "   1.20570888e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.39120252e-11\n",
      "   1.99405700e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.24447336e-10 1.45612536e-10 9.18193646e-11 5.56481007e-11\n",
      "   0.00000000e+00 3.43163276e-11 2.07752912e-10 2.35576947e-10\n",
      "   1.40047723e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.39120252e-11\n",
      "   1.61379493e-10 2.35576947e-10 2.35576947e-10 1.46540002e-10\n",
      "   3.61712639e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.03115566e-10 2.35576947e-10\n",
      "   1.77146450e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   3.33888611e-11 9.08918982e-11 4.63734173e-12 2.78240495e-12\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 9.73841771e-11 2.35576947e-10 2.35576947e-10\n",
      "   1.45612536e-10 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   1.39120252e-11 1.58597094e-10 2.35576947e-10 2.10535311e-10\n",
      "   5.56481007e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.20570888e-11 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   7.51249341e-11 2.12390258e-10 2.35576947e-10 1.92913421e-10\n",
      "   1.02021516e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.01094050e-10 1.96623287e-10 6.58502558e-11\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 6.67777222e-11\n",
      "   2.07752912e-10 2.35576947e-10 2.15172657e-10 4.63734190e-11\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.79928863e-10 2.35576947e-10 2.32794561e-10\n",
      "   1.10368735e-10 1.94768351e-11 1.20570888e-11 1.94768351e-11\n",
      "   1.94768351e-11 2.22592396e-11 1.35410377e-10 2.21664936e-10\n",
      "   2.35576947e-10 2.21664936e-10 1.11296201e-10 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 4.63734190e-11 2.00333167e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.04970499e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.35576947e-10 2.35576947e-10 2.35576947e-10\n",
      "   2.00333167e-10 8.25446864e-11 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.59691140e-11 1.79928863e-10\n",
      "   2.26302282e-10 2.26302282e-10 2.32794561e-10 2.35576947e-10\n",
      "   2.35576947e-10 2.31867081e-10 2.06825446e-10 1.29845565e-10\n",
      "   5.56481007e-11 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 6.02854433e-11 8.71820255e-11\n",
      "   8.71820255e-11 4.91558218e-11 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]], shape=(1, 28, 28), dtype=float32)\n",
      "(1, 28, 28) (1, 28, 28)\n",
      "Tensor(\"strided_slice_2:0\", shape=(), dtype=float32) Tensor(\"strided_slice_5:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"<ipython-input-70-bfacd88dd0a4>\", line 27, in train_step  *\n        loss = compute_loss(model, x)\n    File \"<ipython-input-79-697718f0f7bd>\", line 13, in compute_loss  *\n        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n\n    ValueError: Invalid reduction dimension 3 for input with 3 dimensions. for '{{node Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](logistic_loss, Sum/reduction_indices)' with input shapes: [1,28,28], [3] and with computed input tensors: input[1] = <1 2 3>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-f90112551232>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-70-bfacd88dd0a4>\", line 27, in train_step  *\n        loss = compute_loss(model, x)\n    File \"<ipython-input-79-697718f0f7bd>\", line 13, in compute_loss  *\n        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n\n    ValueError: Invalid reduction dimension 3 for input with 3 dimensions. for '{{node Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](logistic_loss, Sum/reduction_indices)' with input shapes: [1,28,28], [3] and with computed input tensors: input[1] = <1 2 3>.\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for train_x in train_dataset:\n",
    "        print(train_x)\n",
    "        train_step(model, train_x, optimizer)\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, elbo, end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
